#!/usr/bin/env python3
"""
RAGç³»ç»Ÿæµ‹è¯•è„šæœ¬ - RAG System Test Script
Comprehensive testing for the Document Q&A RAG system
"""

import os
import sys
import tempfile
import logging
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ åº”ç”¨è·¯å¾„ - Add app path
sys.path.append(str(Path(__file__).parent / 'app'))

def setup_logging():
    """è®¾ç½®æµ‹è¯•æ—¥å¿— - Setup test logging"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

def create_test_documents() -> List[tuple]:
    """åˆ›å»ºæµ‹è¯•æ–‡æ¡£ - Create test documents"""
    # æµ‹è¯•æ–‡æ¡£å†…å®¹ - Test document content
    test_docs = [
        ("test_document.txt", """
Document Q&A RAG System Test Document

This is a comprehensive test document for the RAG system.

Section 1: Introduction
The RAG (Retrieval-Augmented Generation) system combines the power of 
vector search with large language models to provide accurate answers 
based on document content.

Key features include:
- Multi-file upload support
- Intelligent document chunking
- FAISS vector storage
- Source citation tracking
- Real-time processing metrics

Section 2: Technical Architecture
The system uses the following components:
1. DocumentLoader: Processes PDF, DOCX, and TXT files
2. VectorDatabase: FAISS-based similarity search
3. QAChain: LangChain integration with OpenAI GPT-4o
4. Streamlit UI: Interactive web interface

Section 3: Performance Metrics
Expected performance characteristics:
- Document processing: <30 seconds for typical documents
- Query response time: <5 seconds
- Accuracy: High precision with source citations
- Scalability: Handles multiple large documents efficiently

Section 4: Use Cases
Common applications include:
- Enterprise document search
- Research paper analysis
- Legal document review
- Technical documentation Q&A
- Knowledge base queries
        """),
        
        ("technical_specs.txt", """
Technical Specifications - RAG System

Hardware Requirements:
- CPU: 4+ cores recommended
- RAM: 8GB minimum, 16GB recommended
- Storage: 10GB free space for vector database
- Network: Internet connection for OpenAI API

Software Dependencies:
- Python 3.11+
- LangChain framework
- OpenAI Python library
- FAISS vector search
- Streamlit web framework
- PyMuPDF for PDF processing

Configuration Parameters:
- Chunk Size: 1000 characters (configurable)
- Chunk Overlap: 200 characters
- Vector Dimensions: 1536 (OpenAI embeddings)
- Top-K Retrieval: 5 documents
- Temperature: 0 (deterministic responses)

API Limits:
- OpenAI API rate limits apply
- Recommended: Monitor token usage
- Cost optimization: Use efficient chunking
        """)
    ]
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ - Create temporary files
    temp_files = []
    for filename, content in test_docs:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(content)
            temp_files.append((f.name.encode(), filename))
    
    return temp_files

def test_document_loader():
    """æµ‹è¯•æ–‡æ¡£åŠ è½½å™¨ - Test document loader"""
    logging.info("ğŸ§ª Testing DocumentLoader...")
    
    try:
        from loader import DocumentLoader
        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£ - Create test documents
        test_files = create_test_documents()
        
        # åˆå§‹åŒ–åŠ è½½å™¨ - Initialize loader
        loader = DocumentLoader(chunk_size=500, chunk_overlap=100)
        
        # åŠ è½½æ–‡æ¡£ - Load documents
        documents = loader.load_multiple_files([
            (open(file_path, 'rb').read(), filename) 
            for file_path, filename in test_files
        ])
        
        # éªŒè¯ç»“æœ - Validate results
        assert len(documents) > 0, "No documents loaded"
        assert all(doc.page_content for doc in documents), "Empty document content"
        assert all('file_name' in doc.metadata for doc in documents), "Missing metadata"
        
        logging.info("âœ… DocumentLoader test passed - %d chunks created", len(documents))
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ - Clean up temp files
        for file_path, _ in test_files:
            Path(file_path.decode()).unlink(missing_ok=True)
        
        return documents
        
    except Exception as e:
        logging.error("âŒ DocumentLoader test failed: %s", e)
        return None

def test_vector_database(documents):
    """æµ‹è¯•å‘é‡æ•°æ®åº“ - Test vector database"""
    if not documents:
        logging.error("âŒ No documents provided for VectorDatabase test")
        return None
    
    logging.info("ğŸ§ª Testing VectorDatabase...")
    
    try:
        from vectordb import VectorDatabase
        
        # ä½¿ç”¨ä¸´æ—¶ç›®å½• - Use temporary directory
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆå§‹åŒ–å‘é‡æ•°æ®åº“ - Initialize vector database
            vector_db = VectorDatabase(db_path=temp_dir)
            
            # æ·»åŠ æ–‡æ¡£ - Add documents
            success = vector_db.add_documents(documents)
            assert success, "Failed to add documents"
            
            # ä¿å­˜æ•°æ®åº“ - Save database
            success = vector_db.save_database()
            assert success, "Failed to save database"
            
            # æµ‹è¯•æœç´¢ - Test search
            results = vector_db.search_similar("RAG system architecture", k=3)
            assert len(results) > 0, "No search results found"
            assert all('content' in result for result in results), "Missing content in results"
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯ - Get statistics
            stats = vector_db.get_stats()
            assert stats['document_count'] > 0, "Invalid document count"
            
            logging.info("âœ… VectorDatabase test passed - %d documents indexed", 
                        stats['document_count'])
            
            return vector_db
    
    except Exception as e:
        logging.error("âŒ VectorDatabase test failed: %s", e)
        return None

def test_qa_chain(vector_db):
    """æµ‹è¯•é—®ç­”é“¾ - Test QA chain"""
    if not vector_db:
        logging.error("âŒ No vector database provided for QAChain test")
        return False
    
    logging.info("ğŸ§ª Testing QAChain...")
    
    try:
        from qa_chain import QAChain
        
        # æ£€æŸ¥APIå¯†é’¥ - Check API key
        if not os.getenv('OPENAI_API_KEY'):
            logging.warning("âš ï¸ No OpenAI API key found - skipping QAChain test")
            return True
        
        # åˆå§‹åŒ–QAé“¾ - Initialize QA chain
        qa_chain = QAChain(
            model_name="gpt-4o",
            temperature=0,
            strict_mode=True
        )
        
        # è®¾ç½®æ£€ç´¢å™¨ - Setup retriever
        retriever = vector_db.get_retriever(k=3)
        success = qa_chain.setup_chain(retriever)
        assert success, "Failed to setup QA chain"
        
        # æµ‹è¯•é—®ç­” - Test Q&A
        test_questions = [
            "What are the key features of the RAG system?",
            "What are the hardware requirements?",
            "How does the system process documents?"
        ]
        
        for question in test_questions:
            result = qa_chain.ask_question(question)
            assert not result.get('error'), f"QA failed for: {question}"
            assert result.get('answer'), f"No answer for: {question}"
            assert len(result.get('source_documents', [])) > 0, f"No sources for: {question}"
            
            logging.info("âœ… Q&A test passed: %s -> %s", 
                        question[:50] + "...", 
                        result['answer'][:100] + "...")
        
        logging.info("âœ… QAChain test passed - all questions answered")
        return True
        
    except Exception as e:
        logging.error("âŒ QAChain test failed: %s", e)
        return False

def test_utils():
    """æµ‹è¯•å·¥å…·å‡½æ•° - Test utility functions"""
    logging.info("ğŸ§ª Testing Utils...")
    
    try:
        from utils import (
            Timer, format_file_size, safe_filename, 
            load_config, validate_config, get_system_info
        )
        
        # æµ‹è¯•è®¡æ—¶å™¨ - Test timer
        with Timer() as timer:
            import time
            time.sleep(0.1)
        assert timer.elapsed >= 0.1, "Timer not working"
        
        # æµ‹è¯•æ–‡ä»¶å¤§å°æ ¼å¼åŒ– - Test file size formatting
        assert format_file_size(1024) == "1.0KB", "File size formatting failed"
        assert format_file_size(1024*1024) == "1.0MB", "File size formatting failed"
        
        # æµ‹è¯•å®‰å…¨æ–‡ä»¶å - Test safe filename
        assert safe_filename("test/file?.txt") == "test_file_.txt", "Safe filename failed"
        
        # æµ‹è¯•é…ç½®åŠ è½½ - Test config loading
        config = load_config()
        assert isinstance(config, dict), "Config loading failed"
        
        # æµ‹è¯•é…ç½®éªŒè¯ - Test config validation
        errors = validate_config(config)
        assert isinstance(errors, list), "Config validation failed"
        
        # æµ‹è¯•ç³»ç»Ÿä¿¡æ¯ - Test system info
        sys_info = get_system_info()
        assert 'platform' in sys_info, "System info missing platform"
        
        logging.info("âœ… Utils test passed")
        return True
        
    except Exception as e:
        logging.error("âŒ Utils test failed: %s", e)
        return False

def test_integration():
    """é›†æˆæµ‹è¯• - Integration test"""
    logging.info("ğŸ§ª Running integration test...")
    
    try:
        # ç«¯åˆ°ç«¯æµ‹è¯•æµç¨‹ - End-to-end test flow
        logging.info("1ï¸âƒ£ Testing document loading...")
        documents = test_document_loader()
        if not documents:
            return False
        
        logging.info("2ï¸âƒ£ Testing vector database...")
        vector_db = test_vector_database(documents)
        if not vector_db:
            return False
        
        logging.info("3ï¸âƒ£ Testing QA chain...")
        qa_success = test_qa_chain(vector_db)
        if not qa_success:
            return False
        
        logging.info("4ï¸âƒ£ Testing utilities...")
        utils_success = test_utils()
        if not utils_success:
            return False
        
        logging.info("âœ… Integration test passed - all components working")
        return True
        
    except Exception as e:
        logging.error("âŒ Integration test failed: %s", e)
        return False

def run_performance_test():
    """æ€§èƒ½æµ‹è¯• - Performance test"""
    logging.info("ğŸ§ª Running performance test...")
    
    try:
        from utils import Timer
        
        # åˆ›å»ºå¤§é‡æµ‹è¯•æ–‡æ¡£ - Create multiple test documents
        large_content = "Test content. " * 1000  # ~14KB content
        test_files = [(large_content.encode(), "large_test.txt")]
        
        # æµ‹è¯•æ–‡æ¡£å¤„ç†æ€§èƒ½ - Test document processing performance
        with Timer() as timer:
            from loader import DocumentLoader
            loader = DocumentLoader(chunk_size=1000, chunk_overlap=200)
            documents = loader.load_multiple_files(test_files)
        
        processing_time = timer.elapsed
        logging.info("ğŸ“Š Document processing: %.2fs for %d chunks", 
                    processing_time, len(documents))
        
        # æµ‹è¯•å‘é‡åŒ–æ€§èƒ½ - Test vectorization performance
        if os.getenv('OPENAI_API_KEY'):
            with Timer() as timer:
                from vectordb import VectorDatabase
                with tempfile.TemporaryDirectory() as temp_dir:
                    vector_db = VectorDatabase(db_path=temp_dir)
                    vector_db.add_documents(documents[:10])  # Limit for API costs
            
            vectorization_time = timer.elapsed
            logging.info("ğŸ“Š Vectorization: %.2fs for %d chunks", 
                        vectorization_time, min(10, len(documents)))
        
        logging.info("âœ… Performance test completed")
        return True
        
    except Exception as e:
        logging.error("âŒ Performance test failed: %s", e)
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•° - Main test function"""
    setup_logging()
    
    print("ğŸ§ª RAG System Comprehensive Test Suite")
    print("=" * 50)
    
    # åŠ è½½ç¯å¢ƒå˜é‡ - Load environment variables
    from dotenv import load_dotenv
    load_dotenv()
    
    test_results = []
    
    # è¿è¡Œæµ‹è¯•å¥—ä»¶ - Run test suite
    tests = [
        ("å·¥å…·å‡½æ•°æµ‹è¯•", test_utils),
        ("é›†æˆæµ‹è¯•", test_integration),
        ("æ€§èƒ½æµ‹è¯•", run_performance_test),
    ]
    
    for test_name, test_func in tests:
        logging.info("ğŸ” Running %s...", test_name)
        try:
            result = test_func()
            test_results.append((test_name, result))
            if result:
                logging.info("âœ… %s passed", test_name)
            else:
                logging.error("âŒ %s failed", test_name)
        except Exception as e:
            logging.error("âŒ %s error: %s", test_name, e)
            test_results.append((test_name, False))
    
    # æµ‹è¯•ç»“æœæ€»ç»“ - Test results summary
    print("\nğŸ“Š Test Results Summary")
    print("=" * 30)
    
    passed = sum(1 for _, result in test_results if result)
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{test_name}: {status}")
    
    print(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All tests passed! RAG system is ready for use.")
        return True
    else:
        print("âš ï¸ Some tests failed. Please check the logs above.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 